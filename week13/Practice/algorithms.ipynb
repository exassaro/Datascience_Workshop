{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 supervised learnign algorithms\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 linear regression (regression and classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 4.440892098500626e-16\n",
      "MSE: 3.944304526105059e-31\n",
      "RMSE 6.280369834735101e-16\n",
      "R2: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error,mean_squared_error,r2_score\n",
    "import numpy as np\n",
    "\n",
    "# sample data\n",
    "X=np.array([[1],[2],[3],[4],[5],[6],[7],[8],[9],[10]])\n",
    "y=np.array([2,4,6,8,10,12,14,16,18,20])\n",
    "\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=42)\n",
    "\n",
    "model=LinearRegression()\n",
    "model.fit(X_train,y_train)\n",
    "\n",
    "y_pred=model.predict(X_test)\n",
    "\n",
    "# evaluation\n",
    "print('MAE:',mean_absolute_error(y_test,y_pred))\n",
    "# Since it's almost zero, the model's predictions are nearly perfect.\n",
    "print('MSE:',mean_squared_error(y_test,y_pred))\n",
    "# A near-zero value suggests minimal errors in predictions.\n",
    "print('RMSE',np.sqrt(mean_squared_error(y_test,y_pred)))\n",
    "# very small value suggests almost perfect predictions.\n",
    "print('R2:',r2_score(y_test,y_pred))\n",
    "# A perfect score of 1.0 means the model explains 100% of the variance, meaning it fits the data perfectly.\n",
    "\n",
    "# Your model is overfitting if tested on the training data, as these values are too perfect.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 logistic regression (classification metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 1.0\n",
      "precision: 1.0\n",
      "recall: 1.0\n",
      "F1-score: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score\n",
    "\n",
    "X=[[1],[2],[3],[4],[5],[6],[7],[8],[9],[10]]\n",
    "y=[0,0,0,0,1,1,1,1,1,1,] #binary classification\n",
    "\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=42)\n",
    "\n",
    "model=LogisticRegression()\n",
    "model.fit(X_train,y_train)\n",
    "\n",
    "y_pred=model.predict(X_test)\n",
    "\n",
    "# evaluation\n",
    "print('accuracy:',accuracy_score(y_test,y_pred))\n",
    "# The model correctly classified 100% of the instances.\n",
    "print('precision:',precision_score(y_test,y_pred))\n",
    "# All positive predictions made by the model were correct (no false positives).\n",
    "print('recall:',recall_score(y_test,y_pred))\n",
    "# The model correctly identified all actual positives (no false negatives).\n",
    "print('F1-score:',f1_score(y_test,y_pred))\n",
    "# Since both precision and recall are perfect, their harmonic mean (F1-score) is also 1.0.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 decision tree (classification metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 1.0\n",
      "precision: 1.0\n",
      "recall: 1.0\n",
      "F1-score: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "X= [[1], [2], [3], [4], [5], [6], [7], [8], [9], [10]]\n",
    "y = [0, 0, 0, 0, 1, 1, 1, 1, 1, 1]\n",
    "\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=42)\n",
    "\n",
    "model=DecisionTreeClassifier()\n",
    "model.fit(X_train,y_train)\n",
    "\n",
    "y_pred=model.predict(X_test)\n",
    "\n",
    "print('accuracy:',accuracy_score(y_test,y_pred))\n",
    "print('precision:',precision_score(y_test,y_pred))\n",
    "print('recall:',recall_score(y_test,y_pred))\n",
    "print('F1-score:',f1_score(y_test,y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Random forest (classification metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 1.0\n",
      "precision: 1.0\n",
      "recall: 1.0\n",
      "F1-score: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "X= [[1], [2], [3], [4], [5], [6], [7], [8], [9], [10]]\n",
    "y = [0, 0, 0, 0, 1, 1, 1, 1, 1, 1]\n",
    "\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=42)\n",
    "\n",
    "model=RandomForestClassifier()\n",
    "model.fit(X_train,y_train)\n",
    "\n",
    "print('accuracy:',accuracy_score(y_test,y_pred))\n",
    "print('precision:',precision_score(y_test,y_pred))\n",
    "print('recall:',recall_score(y_test,y_pred))\n",
    "print('F1-score:',f1_score(y_test,y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 SVM (support vm) (classification metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 1.0\n",
      "precision: 1.0\n",
      "recall: 1.0\n",
      "F1-score: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "X= [[1], [2], [3], [4], [5], [6], [7], [8], [9], [10]]\n",
    "y = [0, 0, 0, 0, 1, 1, 1, 1, 1, 1]\n",
    "\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=42)\n",
    "\n",
    "model=SVC()\n",
    "model.fit(X_train,y_train)\n",
    "\n",
    "y_pred=model.predict(X_test)\n",
    "\n",
    "print('accuracy:',accuracy_score(y_test,y_pred))\n",
    "print('precision:',precision_score(y_test,y_pred))\n",
    "print('recall:',recall_score(y_test,y_pred))\n",
    "print('F1-score:',f1_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 KNN (classification metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 1.0\n",
      "precision: 1.0\n",
      "recall: 1.0\n",
      "F1-score: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "X= [[1], [2], [3], [4], [5], [6], [7], [8], [9], [10]]\n",
    "y = [0, 0, 0, 0, 1, 1, 1, 1, 1, 1]\n",
    "\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=42)\n",
    "\n",
    "model=KNeighborsClassifier(n_neighbors=3)\n",
    "model.fit(X_train,y_train)\n",
    "\n",
    "y_pred=model.predict(X_test)\n",
    "\n",
    "print('accuracy:',accuracy_score(y_test,y_pred))\n",
    "print('precision:',precision_score(y_test,y_pred))\n",
    "print('recall:',recall_score(y_test,y_pred))\n",
    "print('F1-score:',f1_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7 Naive Bayes (classification metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 1.0\n",
      "precision: 1.0\n",
      "recall: 1.0\n",
      "F1-score: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "X= [[1], [2], [3], [4], [5], [6], [7], [8], [9], [10]]\n",
    "y = [0, 0, 0, 0, 1, 1, 1, 1, 1, 1]\n",
    "\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=42)\n",
    "\n",
    "model=GaussianNB()\n",
    "model.fit(X_train,y_train)\n",
    "\n",
    "y_pred=model.predict(X_test)\n",
    "\n",
    "print('accuracy:',accuracy_score(y_test,y_pred))\n",
    "print('precision:',precision_score(y_test,y_pred))\n",
    "print('recall:',recall_score(y_test,y_pred))\n",
    "print('F1-score:',f1_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 unsupervised learning (clustering metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 K-Means clustering (silhouette score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "silhouette score: 0.8065476190476191\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import numpy as np\n",
    "\n",
    "X=np.array([[1],[2],[3],[8],[9],[10]])\n",
    "\n",
    "model=KMeans(n_clusters=2,random_state=42)\n",
    "model.fit(X)\n",
    "\n",
    "print('silhouette score:',silhouette_score(X,model.labels_))\n",
    "\n",
    "# The Silhouette Score measures how well clusters are formed in unsupervised learning \n",
    "# It ranges from -1 to 1\n",
    "# 1 → Clusters are well-separated and distinct.\n",
    "# 0 → Clusters are overlapping or ambiguous.\n",
    "# Negative values → Samples are likely misclassified in the wrong cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Heirarchial clustering (silhouette score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "silhouette_score: 0.8065476190476191\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "X=np.array([[1],[2],[3],[8],[9],[10]])\n",
    "\n",
    "model=AgglomerativeClustering(n_clusters=2)\n",
    "model.fit(X)\n",
    "\n",
    "print('silhouette_score:',silhouette_score(X,model.labels_))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "exassaro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
